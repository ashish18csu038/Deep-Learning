{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AMuplKoptNZG"
   },
   "source": [
    "# Lemmatizing Words Using Wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "2r14xyT1tRWd"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UzpyR-dRvQj6",
    "outputId": "de8ac96b-4276-445e-85dd-f8777b4fe223"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jv-tOrXPtp9P",
    "outputId": "66bea216-bdeb-446e-8eff-e92e4964d096"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lodevmyitmPx"
   },
   "source": [
    "# Lemmatizing Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IC0bATELtcOm",
    "outputId": "d7c1250c-3cea-4711-aee3-5dc83697b1ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "definition\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "print(wnl.lemmatize('definitions'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3yYOcVFt1jd"
   },
   "source": [
    "# Lemmatizing words by specifying parts-of-speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqLiNL4atn4k",
    "outputId": "2175429c-9383-49e6-a66d-15cfa9ba975f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjective:  running\n",
      "Adverb:  running\n",
      "Noun:  running\n",
      "Verb:  run\n"
     ]
    }
   ],
   "source": [
    "print('Adjective: ', wnl.lemmatize('running', pos='a'))\n",
    "print('Adverb: ', wnl.lemmatize('running', pos='r'))\n",
    "print('Noun: ', wnl.lemmatize('running', pos='n'))\n",
    "print('Verb: ', wnl.lemmatize('running', pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "onOqC_Sut5JO"
   },
   "outputs": [],
   "source": [
    "input_tokens = ['dictionaries', 'dictionary', \n",
    "                'hushed', 'hush', 'hushing',\n",
    "                'functional', 'functionally',\n",
    "                'lying', 'lied', 'lies',\n",
    "                'flawed', 'flaws', 'flawless', \n",
    "                'friendship', 'friendships', 'friendly', 'friendless', \n",
    "                'definitions', 'definition', 'definitely',  \n",
    "                'the', 'these', 'those',\n",
    "                'motivational', 'motivate', 'motivating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "SbvX3HrRuHdG"
   },
   "outputs": [],
   "source": [
    "ss =  SnowballStemmer('english')\n",
    "\n",
    "ss_stemmed_tokens = []\n",
    "for token in input_tokens:\n",
    "    ss_stemmed_tokens.append(ss.stem(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "5tAyt6WMuIzA"
   },
   "outputs": [],
   "source": [
    "wnl_lemmatized_tokens = []\n",
    "for token in input_tokens:\n",
    "    wnl_lemmatized_tokens.append(wnl.lemmatize(token, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 855
    },
    "id": "2oAtibVOuKfc",
    "outputId": "9d310514-8730-4dee-c2ae-683434a64d2b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>Snowball Stemmer</th>\n",
       "      <th>WordNet Lemmatizer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dictionaries</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionaries</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dictionary</td>\n",
       "      <td>dictionari</td>\n",
       "      <td>dictionary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hushed</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>hushing</td>\n",
       "      <td>hush</td>\n",
       "      <td>hush</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>functional</td>\n",
       "      <td>function</td>\n",
       "      <td>functional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>functionally</td>\n",
       "      <td>function</td>\n",
       "      <td>functionally</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>lying</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>lied</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>lies</td>\n",
       "      <td>lie</td>\n",
       "      <td>lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flawed</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>flaws</td>\n",
       "      <td>flaw</td>\n",
       "      <td>flaw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "      <td>flawless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>friendships</td>\n",
       "      <td>friendship</td>\n",
       "      <td>friendships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>friendly</td>\n",
       "      <td>friend</td>\n",
       "      <td>friendly</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "      <td>friendless</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>definitions</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitions</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>definition</td>\n",
       "      <td>definit</td>\n",
       "      <td>definition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>definitely</td>\n",
       "      <td>definit</td>\n",
       "      <td>definitely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "      <td>the</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "      <td>these</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "      <td>those</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>motivational</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivational</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>motivate</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>motivating</td>\n",
       "      <td>motiv</td>\n",
       "      <td>motivate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           words Snowball Stemmer WordNet Lemmatizer\n",
       "0   dictionaries       dictionari       dictionaries\n",
       "1     dictionary       dictionari         dictionary\n",
       "2         hushed             hush               hush\n",
       "3           hush             hush               hush\n",
       "4        hushing             hush               hush\n",
       "5     functional         function         functional\n",
       "6   functionally         function       functionally\n",
       "7          lying              lie                lie\n",
       "8           lied              lie                lie\n",
       "9           lies              lie                lie\n",
       "10        flawed             flaw               flaw\n",
       "11         flaws             flaw               flaw\n",
       "12      flawless         flawless           flawless\n",
       "13    friendship       friendship         friendship\n",
       "14   friendships       friendship        friendships\n",
       "15      friendly           friend           friendly\n",
       "16    friendless       friendless         friendless\n",
       "17   definitions          definit        definitions\n",
       "18    definition          definit         definition\n",
       "19    definitely          definit         definitely\n",
       "20           the              the                the\n",
       "21         these            these              these\n",
       "22         those            those              those\n",
       "23  motivational            motiv       motivational\n",
       "24      motivate            motiv           motivate\n",
       "25    motivating            motiv           motivate"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stems_lemmas_df = pd.DataFrame({\n",
    "    'words': input_tokens,\n",
    "    'Snowball Stemmer': ss_stemmed_tokens,\n",
    "    'WordNet Lemmatizer': wnl_lemmatized_tokens\n",
    "})\n",
    "\n",
    "stems_lemmas_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xs-Z28oWuMIO",
    "outputId": "dc275819-6443-4bec-f82e-4b13d1cd34bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The advent of computer graphic processing units, improvement in mathematical models and availability of big data has allowed artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques to achieve robust performance for broad applications in social-media, the internet of things, the automotive industry and healthcare. DL systems in particular provide improved capability in image, speech and motion recognition as well as in natural language processing. In medicine, significant progress of AI and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology, pathology and ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are accurate and effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, from digital fundus photographs. There is also increasing attention on the use of AI and DL systems in identifying disease features, progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema using optical coherence tomography (OCT). Additionally, the application of ML to visual fields may be useful in detecting glaucoma progression. There are limited studies that incorporate clinical data including electronic health records, in AL and DL algorithms, and no prospective studies to demonstrate that AI and DL algorithms can predict the development of clinical eye disease. This article describes global eye disease burden, unmet needs and common conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those needs, and the potential challenges for clinical adoption are discussed. AI, ML and DL will likely play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment in the setting of ageing populations globally.\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "with open('DLdata.txt', 'r') as f:\n",
    "    file_contents = f.read()\n",
    "    \n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "-FRxqOVwuRPc"
   },
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "qkjWb_d4vFoT"
   },
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "lemmatized_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    lemmatized_words.append(wnl.lemmatize(word, pos=\"v\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 239
    },
    "id": "QUwJPwr9vVjJ",
    "outputId": "d133b4dc-97fc-4771-c5c6-daeee23337ed"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The advent of computer graphic process units , improvement in mathematical model and availability of big data have allow artificial intelligence ( AI ) use machine learn ( ML ) and deep learn ( DL ) techniques to achieve robust performance for broad applications in social-media , the internet of things , the automotive industry and healthcare . DL systems in particular provide improve capability in image , speech and motion recognition as well as in natural language process . In medicine , significant progress of AI and DL systems have be demonstrate in image-centric specialties such as radiology , dermatology , pathology and ophthalmology . New study , include pre-registered prospective clinical trials , have show DL systems be accurate and effective in detect diabetic retinopathy ( DR ) , glaucoma , age-related macular degeneration ( AMD ) , retinopathy of prematurity , refractive error and in identify cardiovascular risk factor and diseases , from digital fundus photograph . There be also increase attention on the use of AI and DL systems in identify disease feature , progression and treatment response for retinal diseases such as neovascular AMD and diabetic macular edema use optical coherence tomography ( OCT ) . Additionally , the application of ML to visual field may be useful in detect glaucoma progression . There be limit study that incorporate clinical data include electronic health record , in AL and DL algorithms , and no prospective study to demonstrate that AI and DL algorithms can predict the development of clinical eye disease . This article describe global eye disease burden , unmet need and common condition of public health importance for which AI and DL systems may be applicable . Technical and clinical aspects to build a DL system to address those need , and the potential challenge for clinical adoption be discuss . AI , ML and DL will likely play a crucial role in clinical ophthalmology practice , with implications for screen , diagnosis and follow up of the major cause of vision impairment in the set of age populations globally .'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ICnOjGEvbu4"
   },
   "source": [
    "# Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nKru5dqnvY8R",
    "outputId": "5ebae5eb-749e-4673-eb20-71ae05744fdf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jzDXpNYtvn_g",
    "outputId": "78681007-ed01-4ac9-e01d-d27b5608542e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arabic', 'azerbaijani', 'danish', 'dutch', 'english', 'finnish', 'french', 'german', 'greek', 'hungarian', 'indonesian', 'italian', 'kazakh', 'nepali', 'norwegian', 'portuguese', 'romanian', 'russian', 'slovene', 'spanish', 'swedish', 'tajik', 'turkish']\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AVaBSxykwUeQ",
    "outputId": "14eaa22e-3397-4f7d-eb48-8c06633ae17e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "Arabic:  ['إذ', 'إذا', 'إذما', 'إذن', 'أف', 'أقل', 'أكثر', 'ألا', 'إلا', 'التي', 'الذي', 'الذين', 'اللاتي', 'اللائي', 'اللتان', 'اللتيا', 'اللتين', 'اللذان', 'اللذين', 'اللواتي', 'إلى', 'إليك', 'إليكم', 'إليكما', 'إليكن', 'أم', 'أما', 'أما', 'إما', 'أن', 'إن', 'إنا', 'أنا', 'أنت', 'أنتم', 'أنتما', 'أنتن', 'إنما', 'إنه', 'أنى', 'أنى', 'آه', 'آها', 'أو', 'أولاء', 'أولئك', 'أوه', 'آي', 'أي', 'أيها', 'إي', 'أين', 'أين', 'أينما', 'إيه', 'بخ', 'بس', 'بعد', 'بعض', 'بك', 'بكم', 'بكم', 'بكما', 'بكن', 'بل', 'بلى', 'بما', 'بماذا', 'بمن', 'بنا', 'به', 'بها', 'بهم', 'بهما', 'بهن', 'بي', 'بين', 'بيد', 'تلك', 'تلكم', 'تلكما', 'ته', 'تي', 'تين', 'تينك', 'ثم', 'ثمة', 'حاشا', 'حبذا', 'حتى', 'حيث', 'حيثما', 'حين', 'خلا', 'دون', 'ذا', 'ذات', 'ذاك', 'ذان', 'ذانك', 'ذلك', 'ذلكم', 'ذلكما', 'ذلكن', 'ذه', 'ذو', 'ذوا', 'ذواتا', 'ذواتي', 'ذي', 'ذين', 'ذينك', 'ريث', 'سوف', 'سوى', 'شتان', 'عدا', 'عسى', 'عل', 'على', 'عليك', 'عليه', 'عما', 'عن', 'عند', 'غير', 'فإذا', 'فإن', 'فلا', 'فمن', 'في', 'فيم', 'فيما', 'فيه', 'فيها', 'قد', 'كأن', 'كأنما', 'كأي', 'كأين', 'كذا', 'كذلك', 'كل', 'كلا', 'كلاهما', 'كلتا', 'كلما', 'كليكما', 'كليهما', 'كم', 'كم', 'كما', 'كي', 'كيت', 'كيف', 'كيفما', 'لا', 'لاسيما', 'لدى', 'لست', 'لستم', 'لستما', 'لستن', 'لسن', 'لسنا', 'لعل', 'لك', 'لكم', 'لكما', 'لكن', 'لكنما', 'لكي', 'لكيلا', 'لم', 'لما', 'لن', 'لنا', 'له', 'لها', 'لهم', 'لهما', 'لهن', 'لو', 'لولا', 'لوما', 'لي', 'لئن', 'ليت', 'ليس', 'ليسا', 'ليست', 'ليستا', 'ليسوا', 'ما', 'ماذا', 'متى', 'مذ', 'مع', 'مما', 'ممن', 'من', 'منه', 'منها', 'منذ', 'مه', 'مهما', 'نحن', 'نحو', 'نعم', 'ها', 'هاتان', 'هاته', 'هاتي', 'هاتين', 'هاك', 'هاهنا', 'هذا', 'هذان', 'هذه', 'هذي', 'هذين', 'هكذا', 'هل', 'هلا', 'هم', 'هما', 'هن', 'هنا', 'هناك', 'هنالك', 'هو', 'هؤلاء', 'هي', 'هيا', 'هيت', 'هيهات', 'والذي', 'والذين', 'وإذ', 'وإذا', 'وإن', 'ولا', 'ولكن', 'ولو', 'وما', 'ومن', 'وهو', 'يا']\n"
     ]
    }
   ],
   "source": [
    "print(\"English: \",stopwords.words('english'))\n",
    "print(\"Arabic: \",stopwords.words('arabic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "pDlpfIZYwQue",
    "outputId": "80720121-e72c-400d-aeef-3de6eeaf6882"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A bird in hand is worth two in the bush. Good things come to those who wait. These watches cost $1500!  There are other fish in the sea. The ball is in your court. Mr. Smith Goes to Washington  Doogie Howser M.D.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_array = [\"A bird in hand is worth two in the bush.\",\n",
    "              \"Good things come to those who wait.\",\n",
    "              \"These watches cost $1500! \",\n",
    "              \"There are other fish in the sea.\",\n",
    "              \"The ball is in your court.\",\n",
    "              \"Mr. Smith Goes to Washington \",\n",
    "              \"Doogie Howser M.D.\"]\n",
    "\n",
    "text = \" \".join(text_array)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BkZm8RQWwtGu",
    "outputId": "76671252-4873-4201-ab4e-3c807417b616"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'bird',\n",
       " 'in',\n",
       " 'hand',\n",
       " 'is',\n",
       " 'worth',\n",
       " 'two',\n",
       " 'in',\n",
       " 'the',\n",
       " 'bush',\n",
       " '.',\n",
       " 'Good',\n",
       " 'things',\n",
       " 'come',\n",
       " 'to',\n",
       " 'those',\n",
       " 'who',\n",
       " 'wait',\n",
       " '.',\n",
       " 'These',\n",
       " 'watches',\n",
       " 'cost',\n",
       " '$',\n",
       " '1500',\n",
       " '!',\n",
       " 'There',\n",
       " 'are',\n",
       " 'other',\n",
       " 'fish',\n",
       " 'in',\n",
       " 'the',\n",
       " 'sea',\n",
       " '.',\n",
       " 'The',\n",
       " 'ball',\n",
       " 'is',\n",
       " 'in',\n",
       " 'your',\n",
       " 'court',\n",
       " '.',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " 'Goes',\n",
       " 'to',\n",
       " 'Washington',\n",
       " 'Doogie',\n",
       " 'Howser',\n",
       " 'M.D',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens = word_tokenize(text)\n",
    "\n",
    "word_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kIupMi94wvYQ",
    "outputId": "0c061f3f-106b-4d56-81eb-839d4da81f79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['A', 'bird', 'hand', 'worth', 'two', 'bush', '.', 'Good', 'things', 'come', 'wait', '.', 'These', 'watches', 'cost', '$', '1500', '!', 'There', 'fish', 'sea', '.', 'The', 'ball', 'court', '.', 'Mr.', 'Smith', 'Goes', 'Washington', 'Doogie', 'Howser', 'M.D', '.']\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_words = []\n",
    "\n",
    "for word in word_tokens:\n",
    "    if word not in stop_words:\n",
    "        filtered_words.append(word)\n",
    "        \n",
    "print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "S_wnXbbjwyWx"
   },
   "outputs": [],
   "source": [
    "with open(\"DLdata.txt\", \"w\") as f:\n",
    "    for word in filtered_words:\n",
    "        f.write(word)\n",
    "        f.write(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AM6-s8u1w8NY",
    "outputId": "fc375a61-9607-4713-e833-2c3e8e86d22e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A bird hand worth two bush . Good things come wait . These watches cost $ 1500 ! There fish sea . The ball court . Mr. Smith Goes Washington Doogie Howser M.D . \n"
     ]
    }
   ],
   "source": [
    "with open(\"DLdata.txt\", \"r\") as f:\n",
    "    file_contents = f.read()\n",
    "\n",
    "print(file_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bZk5qum_w_z_",
    "outputId": "2f69e0f8-0646-4c4c-9dfb-550a5affbf14"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "count_vectorizer.fit([file_contents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcsGnZP1xCbC",
    "outputId": "d5d32a79-4e38-463a-df7a-16837380cbfe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 25)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector = count_vectorizer.transform(text_array)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "BrRMBvXNxEIO"
   },
   "outputs": [],
   "source": [
    "feature_names_nltk = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bHLJZwcxFaE",
    "outputId": "63869962-1eba-41b1-9d43-c3d8bbbc1325"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird': 2,\n",
       " 'hand': 11,\n",
       " 'worth': 24,\n",
       " 'two': 20,\n",
       " 'bush': 3,\n",
       " 'good': 10,\n",
       " 'things': 19,\n",
       " 'come': 4,\n",
       " 'wait': 21,\n",
       " 'these': 18,\n",
       " 'watches': 23,\n",
       " 'cost': 5,\n",
       " '1500': 0,\n",
       " 'there': 17,\n",
       " 'fish': 8,\n",
       " 'sea': 14,\n",
       " 'the': 16,\n",
       " 'ball': 1,\n",
       " 'court': 6,\n",
       " 'mr': 13,\n",
       " 'smith': 15,\n",
       " 'goes': 9,\n",
       " 'washington': 22,\n",
       " 'doogie': 7,\n",
       " 'howser': 12}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3xPrPNG1xHfJ",
    "outputId": "c9335e5a-f1cf-4711-8b78-b756cc0b97df"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeonNdPPxK7x",
    "outputId": "24dc99c5-69d9-4ef1-8110-d98d4846fc42"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['bird', 'bush', 'hand', 'the', 'two', 'worth'], dtype='<U10'),\n",
       " array(['come', 'good', 'things', 'wait'], dtype='<U10'),\n",
       " array(['1500', 'cost', 'these', 'watches'], dtype='<U10'),\n",
       " array(['fish', 'sea', 'the', 'there'], dtype='<U10'),\n",
       " array(['ball', 'court', 'the'], dtype='<U10'),\n",
       " array(['goes', 'mr', 'smith', 'washington'], dtype='<U10'),\n",
       " array(['doogie', 'howser'], dtype='<U10')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YxfRUhhLxNvX"
   },
   "source": [
    "# Removing Stpwords Using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-XQyePXxMsa",
    "outputId": "58d9235c-c82f-4b0e-a6ea-9dc272b14e31"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 21)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(text_array)\n",
    "\n",
    "transformed_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "iGjqOb-CxTbE"
   },
   "outputs": [],
   "source": [
    "feature_names_sklearn = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfOu-dh8xVw7",
    "outputId": "bbb92dde-23bc-4cd9-af9c-b9cd820d3619"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TR6bx6YMxXV2",
    "outputId": "f473eae0-511e-4b90-aea5-f26271eae434"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array(['bird', 'hand', 'worth', 'bush'], dtype='<U10'),\n",
       " array(['good', 'things', 'come', 'wait'], dtype='<U10'),\n",
       " array(['watches', 'cost', '1500'], dtype='<U10'),\n",
       " array(['fish', 'sea'], dtype='<U10'),\n",
       " array(['ball', 'court'], dtype='<U10'),\n",
       " array(['mr', 'smith', 'goes', 'washington'], dtype='<U10'),\n",
       " array(['doogie', 'howser'], dtype='<U10')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer.inverse_transform(transformed_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vhk5eAtAxcoe"
   },
   "source": [
    "# Set Difference of Both\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "BBeWV-YFxZAs"
   },
   "outputs": [],
   "source": [
    "def set_diff(first, second):\n",
    "        second = set(second)\n",
    "        return [item for item in first if item not in second]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i4ACOu9uxa73",
    "outputId": "a9b1c196-2b21-4687-8b83-a0f700bc2c6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_diff(feature_names_sklearn, feature_names_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KwjTG4AXxir7",
    "outputId": "12465d78-8258-415f-e2a6-2d313551a960"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', 'there', 'these', 'two']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set_diff(feature_names_nltk, feature_names_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9N-7bl3xmk2"
   },
   "source": [
    "# Filtering words based on frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "syqplXmfxj8v",
    "outputId": "3e465bce-e8c5-4b44-df6a-38a0807b4278"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z5ztVt7axoa9",
    "outputId": "40d3256a-141d-4eae-b0b2-9305fbbf589c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pILvs8DDxrY9",
    "outputId": "93524777-94a2-4fb0-c5bc-1ee1d7d30669"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(newsgroups.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24_e2jYSxtqo",
    "outputId": "1a3b306b-b5f5-4cb4-f70f-8090b7a737c7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ss963Q5cxva8",
    "outputId": "f981a852-32a7-4b62-91b3-3e4cfaed69a5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 130107)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "\n",
    "transformed_vector = count_vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "transformed_vector.shape"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "22_15.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
