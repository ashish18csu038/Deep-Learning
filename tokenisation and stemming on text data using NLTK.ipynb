{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F83eSqgJUPIm"
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kOpW8MpvUUd2"
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avDUXguHVWts",
    "outputId": "9a2aa159-b7ec-420b-874a-c171596352af"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fyOpYQV2UlM3",
    "outputId": "f380f0e9-b0a3-490c-885f-ccc56e85750d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' Before the extensive use of deep learning, many problems are solved using machine learning but did not get the results as expected.', 'With the boom in technology specially in Graphical Processing Units(GPUs), deep learning comes into picture and solved many complicated problems with the help of artificial neural networks.', 'One of the most important aspect of deep learning is that no feature extraction is required.', 'The model itself manages the weight and extract the best features and then training is done whereas in machine learning, feature extraction is always done prior model training.']\n"
     ]
    }
   ],
   "source": [
    "sent = sent_tokenize(\" Before the extensive use of deep learning, many problems are solved using machine learning but did not get the results as expected. With the boom in technology specially in Graphical Processing Units(GPUs), deep learning comes into picture and solved many complicated problems with the help of artificial neural networks. One of the most important aspect of deep learning is that no feature extraction is required. The model itself manages the weight and extract the best features and then training is done whereas in machine learning, feature extraction is always done prior model training. \")\n",
    "print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zP4xJDS-WmYi",
    "outputId": "43543afd-2892-4fa1-9fa5-2850732d72d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UKCRQ8fqVUHf",
    "outputId": "2e9b89e3-2054-4fad-8094-3ed70e629751"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Many', 'problems', 'are', 'solved', 'using', 'machine', 'learning']\n"
     ]
    }
   ],
   "source": [
    "word = word_tokenize('Many problems are solved using machine learning')\n",
    "\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TzqmCJcqV4vO",
    "outputId": "3f000cfa-5dec-44d9-d2dd-26cff98129bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "YqgMK9y0WPvn"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.punkt import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OoW00TVOlATl"
   },
   "outputs": [],
   "source": [
    "pst = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-c-dyFkGlImh",
    "outputId": "c9429d64-ab92-450b-8bb9-0bf60031c89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Before the extensive use of deep learning, many problems are solved using machine learning but did not get the results as expected.', 'With the boom in technology specially in Graphical Processing Units(GPUs), deep learning comes into picture and solved many complicated problems with the help of artificial neural networks.', 'One of the most important aspect of deep learning is that no feature extraction is required.', 'The model itself manages the weight and extract the best features and then training is done whereas in machine learning, feature extraction is always done prior model training.']\n"
     ]
    }
   ],
   "source": [
    "punkt_sentence = pst.tokenize(\"Before the extensive use of deep learning, many problems are solved using machine learning but did not get the results as expected. With the boom in technology specially in Graphical Processing Units(GPUs), deep learning comes into picture and solved many complicated problems with the help of artificial neural networks. One of the most important aspect of deep learning is that no feature extraction is required. The model itself manages the weight and extract the best features and then training is done whereas in machine learning, feature extraction is always done prior model training. \")\n",
    "print(punkt_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yjq9beJFler0",
    "outputId": "a5970461-06af-4b06-bfa9-0bbbe8de30ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(punkt_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WJFwKOlxl6vQ",
    "outputId": "ae333b8d-3043-4422-82dc-05b7989e789c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 131), (132, 321), (322, 414), (415, 591)]\n"
     ]
    }
   ],
   "source": [
    "span_sentence = pst.span_tokenize(\"Before the extensive use of deep learning, many problems are solved using machine learning but did not get the results as expected. With the boom in technology specially in Graphical Processing Units(GPUs), deep learning comes into picture and solved many complicated problems with the help of artificial neural networks. One of the most important aspect of deep learning is that no feature extraction is required. The model itself manages the weight and extract the best features and then training is done whereas in machine learning, feature extraction is always done prior model training. \")\n",
    "print(list(span_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VEwlSVQWlywl",
    "outputId": "3164abba-4157-48e9-d9dc-e88d346a3b94"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Many', 'problems', 'are', 'solved', 'using', 'machine', 'learning']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = pst.sentences_from_tokens(word)\n",
    "list(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "wBceZEBkm2L0"
   },
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fGI7yvMgnlYb",
    "outputId": "b023111f-3c17-4b6e-bf7d-6d2565c22549"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Many', 1),\n",
       " ('problems', 1),\n",
       " ('are', 1),\n",
       " ('solved', 1),\n",
       " ('using', 1),\n",
       " ('machine', 1),\n",
       " ('learning', 1)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = FreqDist(word)\n",
    "freq.most_common()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LAhXV8Kjog6w"
   },
   "source": [
    "# Stemming\n",
    "\n",
    "  Often when searching text for a certain keyword, it helps if the search returns variations of the word. For instance, searching for \"boat\" might also return \"boats\" and \"boating\". Here, \"boat\" would be the stem for [boat, boater, boating, boats].\n",
    "\n",
    "Stemming is a somewhat crude method for cataloging related words; it essentially chops off letters from the end until the stem is reached. This works fairly well in most cases, but unfortunately English has many exceptions where a more sophisticated process is required. In fact, spaCy doesn't include a stemmer, opting instead to rely entirely on lemmatization. For those interested, there's some background on this decision here. We discuss the virtues of lemmatization in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mypjv0sOqWxB"
   },
   "source": [
    "# Porter Stemmer\n",
    "One of the most common - and effective - stemming tools is Porter's Algorithm developed by Martin Porter in 1980. The algorithm employs five phases of word reduction, each with its own set of mapping rules. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "cWXB7VVfn1Yr"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "uDwjGWsPoztp"
   },
   "outputs": [],
   "source": [
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "ECa7gWLfo3LW"
   },
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly', 'accordingly', 'consolingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "38TT6NpzpBpS",
    "outputId": "da1fd5fa-6680-4f4b-8c9d-8e97eb5f62e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run -->  run\n",
      "runner -->  runner\n",
      "running -->  run\n",
      "ran -->  ran\n",
      "runs -->  run\n",
      "easily -->  easili\n",
      "fairly -->  fairli\n",
      "accordingly -->  accordingli\n",
      "consolingly -->  consolingli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "  print(word+' -->  '+p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C3IHR-ubqDaY"
   },
   "source": [
    "# Snowball Stemmer\n",
    "\n",
    "This is somewhat of a misnomer, as Snowball is the name of a stemming language developed by Martin Porter. The algorithm used here is more acurately called the \"English Stemmer\" or \"Porter2 Stemmer\". It offers a slight improvement over the original Porter stemmer, both in logic and speed. Since nltk uses the name SnowballStemmer, we'll use it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "SKeDkMC7qE3f"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "nHHirthHqwBO"
   },
   "outputs": [],
   "source": [
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly', 'accordingly', 'consolingly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NwzNCQI9q2aL",
    "outputId": "7a8a629f-5afd-42cf-dcf5-6890fde71d69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run -->  run\n",
      "runner -->  runner\n",
      "running -->  run\n",
      "ran -->  ran\n",
      "runs -->  run\n",
      "easily -->  easili\n",
      "fairly -->  fair\n",
      "accordingly -->  accord\n",
      "consolingly -->  consol\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "  print(word+' -->  '+s_stemmer.stem(word))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "21/14.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
